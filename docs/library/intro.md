# Интернет, о котором мало кто знает

==TODO: english version==

## Назад в Будущее

Прежде чем перейти к детальному описанию проекта, мы хотим сделать акцент на важных исторических фактах и предложить вам разобраться в некоторых причинно-следственных связях, относящихся к созданию и развитию сети Интернет, которые будут иметь самое непосредственное значение для нашего проекта, учитывая что основной целью проекта Ace Stream является, немного-немало, - а «Исправление» (Усовершенствование) всего Интернета! То есть, в рамках проекта Ace Stream мы предоставим необходимые технологии, инструменты и стратегии, позволяющие сделать Интернет таким, каким он изначально планировался быть и каким он наконец-то должен стать!


!!! info ""
    "Кажется немыслимым, что сети Интернет уже более 25 лет, и многие из нас едва ли могут представить себе жизнь без нее. Она была создана усилиями миллионов. Мы все помогли построить это, и будущее Интернета все еще зависит от нас. Все мы должны использовать свой творческий потенциал, навыки и опыт, чтобы сделать его лучше: более сильным, более безопасным, более справедливым и более открытым. Давайте выберем сеть, которую мы хотим, и, таким образом, мир, который мы хотим", - [Sir Tim Berners-Lee][1] (изобретатель сети Интернет)

    История и неоспоримые факты позволят нам увидеть истинную реальность, дав нам понимание происходящего и возможность максимально ясно видеть варианты обозримого будущего, определяя нужные нам цели и выбирая наилучшие пути их достижения.

    "Весь потенциал Интернета только начинает проявляться. Радикально открытая, эгалитарная и децентрализованная платформа, она меняет мир, и мы все еще только царапаем поверхность того, что она может сделать. Любой, кто интересуется будущим Интернет - а это все, и везде, - играет определенную роль в обеспечении того, чтобы Сеть достигала всего, что может", - [Sir Tim Berners-Lee][1]

**Тогда...**

23 августа 1991 года британский ученый Тимоти Бернерс-Ли изменил мир, предоставив разработанную им систему «World Wide Web», открыв первый на планете интернет-сайт ([info.cern.ch/][2]). Миру был подарен  Интернет (без патентов, авторских прав или товарных знаков).

Гениальность такого изобретения как Интернет, заключалась не только в его социальной ценности, а еще и в его архитектуре и технологических свойствах, заложенных в фундаментальную основу его эксплуатации и развития.

Всемирная паутина изначально не имела единого центра. Одним из ключевых свойств WWW всегда считалась децентрализация узлов. Как и у прародителей интернета (сетей ARPANET и NSFNet), она обеспечивала надёжность функционирования, отсутствие географических границ и сетевых барьеров. Протокол HTTP соединяет все вычислительные устройства на планете, имеющие подключение к Интернету. В своей работе протокол HTTP опирается на множество доверительных серверов, преобразующих веб-адреса в сетевые адреса серверов.

Логика, заложенная в работу сети Интернет, подразумевала следующее: Люди, желавшие что-то опубликовать онлайн, должны были иметь собственные веб-сервера на своих компьютерах, а обслуживающие Сеть компании должны были выполнять исключительно технические функции, обеспечивая надежное соединение между всеми узлами Сети (строя магистрали и соединяя кабелем компьютеры пользователей Сети).

То есть, Интернет изначально был создан и функционировал как одноранговая (P2P, peep to peer) сеть, где каждый ее участник являлся полноценным узлом сети (клиентом/сервером). Пользователь публиковал информацию непосредственно на своем компьютере, а другой получал эту информацию напрямую, от того кто ее публиковал. Полученную информацию каждый мог продублировать (скопировать/опубликовать) на своем компьютере и поделиться ей с другими, тем самым способствуя ее распространению и обеспечивая других участникам сети возможностью выбора более удобного и быстрого (ближайшего) источника получения данных.

К примеру, в такой Сети, - если бы ваш сосед захотел опубликовать свое видео, ему бы не пришлось отправлять его на сайт/сервис какого-то посредника, сервер которого находится на другом континенте, а для его просмотра вам бы не пришлось тянуть сотни мегабайт или гигабайты данных с другого континента, как это осуществляется при использовании централизованных систем, таких как YouTube, Facebook, и т.п.  Вы бы просто получили это видео напрямую от этого соседа или от другого вашего ближайшего соседа, посмотревшего и сохранившего это видео (находящегося с вами в одном доме или на одной улице, или в одном городе или стране).

Изначально, это была платформа, позволяющая всем напрямую обмениваться информацией, без участия каких-либо центральных сервисов/серверов посредников. В такой одноранговой (P2P) сети, информация имеющая  большую востребованность  находилась на большем количестве компьютеров, а менее востребованная на меньшем, и при этом максимально аккумулировалась на узлах в тех географических точках, где ее больше всего потребляли и соответственно все данные проходили по самым коротким  маршрутам, не перегружая  трафиком международные транспортные магистрали и другие удаленные сети.  Пропускная способность такой одноранговой сети, в первую очередь исчислялось суммарной пропускной способностью подключённых к Сети компьютеров (узлов), и рост ее производительности был практически всегда пропорционален росту количества пользователей сети и объемам производимой/потребляемой информации.

Вот именно такой неограниченный Интернет подарили миру его Основатели!


<p style="text-align: right">
    <strong>
        <em>
            «Интернет - радикально открытая, эгалитарная и децентрализованная платформа»<br/>
            <a href="http://www.wired.co.uk/article/tim-berners-lee">Sir Tim Berners-Lee</a>
        </em>
    </strong>
</p>

_Ну как говорится: «Что один человек построил - другой завсегда сломать сможет»!_


## Эволюция сети Интернет или что пошло не так

Было бы логично предположить, что следующим эволюционным этапом развития такого децентрализованного Интернета являлось бы создание новых и эффективных децентрализованных технологий, которые обеспечивали бы совершенствование и развитие Сети в полном соответствии с заложенными в нее фундаментальными основами (технологическими и социальными). И вероятно, если бы еще один выдающийся ученый из Чикаго, по имени Тед Нельсон, смог своевременно выпустить в свет своей проект Xanadu, который он создал в 1960 году с целью создания системы компьютерной сети с простым пользовательским интерфейсом, у нас бы сейчас был совершенный «Интернет».

17 правил Xanadu (проект «Интернета» от 1960 года)

1. Каждый сервер Xanadu уникальным образом безопасно идентифицируется.
2. Каждый сервер Xanadu может управляться независимо, либо внутри сети.
3. Каждый пользователь уникальным образом безопасно идентифицируется.
4. Каждый пользователь может искать, скачивать, создавать и хранить документы.
5. Каждый документ может состоять из любого количества частей, каждая из которых может быть данными любого типа.
6. Каждый документ может содержать ссылки любого типа, включая виртуальные копии («включения») любого другого документа в системе, доступного владельцу.
7. Ссылки видимы и по ним можно проследовать из любой конечной точки.
8. Разрешение ссылаться на документ явно предоставляется актом публикации.
9. Каждый документ может содержать механизм вознаграждения автора с любой степенью подробности для обеспечения выплаты по частичному чтению документа, в том числе и для («включений») любой части документа.
10. Каждый документ уникальным образом безопасно идентифицируется.
11. Каждый документ может иметь защиту для контроля доступа.
12. Каждый документ может быть найден быстро, храниться и скачиваться без знания пользователем физического положения документа.
13. Каждый документ автоматически перемещается в физическое хранилище, соответствующее частоте доступа к нему из любой данной точки.
14. Каждый документ автоматически сохраняется с избыточностью, позволяющей сохранить доступ к нему даже в случае катастрофы.
15. Каждый провайдер услуг Xanadu может взимать плату со своих пользователей по любым выбранным им ставкам, которые они устанавливают для хранения, поиска и публикации документов.
16. Каждая транзакция безопасна и поддаётся проверке только теми лицами, которые её совершают.
17. Протокол коммуникации клиент-сервер Xanadu — открыто опубликованный стандарт. Разработка и интеграция программного обеспечения сторонними разработчиками поощряется.

Наличие всех этих правил в изначально созданной сети «Интернет» практически сводили бы на нет все попытки централизации Сети, так как в этом просто не было бы никакой практической пользы.

<p style="text-align: right">
    <strong><em>Таким должен был стать Интернет!</em></strong>
</p>


**Почему не вышло?**

Нельсон был гением. Достаточно сказать, что он разработал большую часть принципов, использующихся в нынешнем интернете. Но, как у многих гениальных людей, вероятно у него были серьёзные проблемы с реализацией своих идей.



![alt_text]({{ assets_root }}/images/intro/image1.jpg "image_tooltip")


Он обещал завершить работу над Xanadu сначала к 1979 году, потом к 1987-му. А в 1987-м назвал «крайним сроком» окончания работ 1988‑й.

В 1988 перспективный проект купила компания Autodesk.


!!! info ""
    Её владелец Джон Уокер рассчитывал, что в «1989 году Xanadu станет продуктом, а в 1995 году начнёт переделывать мир».

В действительности на реализацию идей Нельсона потребовалось больше полувека. Только в апреле 2014 года, постаревший, но всё ещё пылающий энтузиазмом первооткрыватель провёл презентацию окончательной версии своего детища — OpenXanadu — в стенах Чепменского университета в Калифорнии.

Случись открытие создателя гипертекста в конце семидесятых или даже в середине девяностых годов — мы сейчас пользовались бы совсем другим интернетом. Но Нельсон продемонстрировал миру свой «волшебный дворец» слишком поздно. Человечество уже тридцать лет как жило с сетью, использующей протоколы TCP/IP, HTTP, язык HTML и идентификаторы URL.


!!! cite ""
    "Хорошей новостью является то, что в структуру Сети изначально заложены открытость и гибкость. Протоколы и языки программирования находящиеся под капотом - включая URL-адреса, TCP/IP, HTTP, HTML, JavaScript и многие другие - почти все были разработаны для эволюции, поэтому мы можем модернизировать их по мере появления новых потребностей, новых устройств и новых бизнес-моделей, устраняя текущие ограничения», - [Sir Tim Berners-Lee][1]

В общем, пока руководство Autodesk настойчиво и безуспешно пыталось добиться от создателя Xanadu завершения работ, сотрудник европейской лаборатории CERN Тим Бернерс-Ли предложил свой глобальный гипертекстовый проект, получивший название World Wide Web, который был более упрощенной версией Xanadu, не имеющей тех возможностей, которые бы изначально делали Сеть более простой и удобной в эксплуатации, а применение многих централизованных технологий было бы не целесообразными.

![alt_text]({{ assets_root }}/images/intro/image2.jpg "image_tooltip")

Ввиду того что созданная Сеть (WWW) в своем первичном виде требовала от пользователей хорошего понимания технологий, а последующая реализация необходимых децентрализованных технологий для эффективной работы в такой сети Интернет являлось очень сложной задачей, на разработку которых нужно было еще потратить многие годы, развитее интернета пошло более простым путем, как на программном уровне, так идеологически, и в абсолютно противоположном направлении. Активное продвижение изначальных принципов децентрализации, составляющих фундаментальную основу творения Тимоти Бернерс-Ли, никак не вписывалось в консервативную политику коммерческих компаний, решивших использовать сеть в коммерческих интересах и для которых централизация является единственной и понятной моделью работы. А еще это было самым простым техническим решением в реализации, чтобы это все нормально работало. Безусловно, все понимали и понимают что скорость и эффективная масштабируемость, а также конфиденциальность и прозрачность являются важнейшими составляющими Сети, но в конце концов, именно деньги заставляют мир вращаться.

По мере разрастания Сети, появились компании, которые взяли на себя техническую заботу о веб-публикациях и коммуникациях.  Одними из первых популярных централизованных приложений(сервисов) стали MySpace и Yahoo!. C помощью программы Flickr фотограф мог с легкостью загрузить свои фотографии в Интернет и поделиться ими с другими людьми. YouTube делал то же самое для видео, а такие инструменты, как Wordpress и Wiki, позволяли любому желающему вести свой собственный блог и/или совместно участвовать в создании публичных информационных баз знаний. Социальные сети, в частности, позволили каждому быть онлайн, чтобы удобным способом общаться и обмениваться информацией с другими пользователями. Все это осуществлялось посредством сайтов и сервисов посредников, предоставлявших для этого необходимую инфраструктуру, сформированную на базе своих центральных серверов. Коммерческие компании стали брать под свой полный контроль все пользовательские публикации, завязывая на свои сервисы/сервера все коммуникации и любой обмен информацией между пользователями. Такое активное коммерческое проникновение в Интернет, вместе с развитием сопутствующих централизованных технологий и удобных сервисов, начало кардинально менять фундаментальные принципы и логику первоначального Интернета, и подвергло серьезному и очень опасному изменению логистическую составляющую Сети.

В отличии от логики первоначального (децентрализованного) интернета, где подразумевалось управление пользователями всей своей информацией и публикациями, с возможностью передачи друг-другу данных по самому короткому маршруту (пути), в новую логику Сети закладывались кардинально противоположные логистические механизмы, согласно которым: Вашему соседу желающему показать вам свое видео нужно вначале отправить его на сервер посредника который мог находиться на другом континенте, а для просмотра этого видео вам нужно загрузить его себе уже непосредственно от сервера этого посредника,  а не напрямую от соседа или от каких-то других ваших ближайших соседей уже посмотревших это видео.  Кроме этого, с целью извлечения максимальной прибыли, сервисы посредников стали строго-настрого запрещать вам сохранять это видео (дублировать экземпляр) у себя на компьютере, и при желании его еще раз посмотреть или поделиться им с другими,  нужно его опять загружать с сервера посредника, и в результате, все многократно тянут одно и тоже видео непосредственно с его сервера находящегося на другом континенте.

В итоге, мир получил «интернет-наоборот», который многими стал называться - Web 2.0, суть и логика которого заключается в следующем:  - чем больше людей сидят на одном web-ресурсе и центральных серверах посредника, осуществляя через него все свои коммуникации (получают информацию, размещают свои публикации, обмениваются данными, общаются и т.д.), тем лучше!  В итоге:


!!! cite ""
    "За последние двадцать лет мы умудрились практически погубить одну из самых функциональных дистрибьютивных систем, которые когда-либо были созданы: современную Сеть"

    &mdash; Кори Доктороу (Cory Doctorow), директор европейского отделения правозащитной организации Electronic Frontiers Foundation’s

Это заявление может показаться очень странным, так как для многих Интернет стал неотъемлемой частью современной жизни. Это портал, через который мы узнаем новости и находим развлечения, поддерживаем контакт с семьей и друзьями, получаем свободный доступ к большему количеству информации, чем все люди, когда-либо жившие до нас. Сегодня Сеть, возможно, более полезна и доступна людям, чем когда-либо. Однако такие люди, как Сэр Тим Бернерс-Ли (Sir Tim Berners-Lee), изобретатель Всемирной Паутины, и Винтон Серф (Vinton Cerf), часто называемый одним из "отцов Интернета", в комментарии Доктороу видят суть проблемы и последствия, которые неизвестны и непонятны большинству: Интернет развивается не так, как они это себе представляли! Использование Сети в централизованном режиме отрицательно влияет на ее производительность в целом, создавая колоссальной дефицит пропускной способности и приводит к раздроблению единой и открытой для всех Сети на частные корпоративные сегменты (частные сети поверх Сети), с ограниченными возможностями и факторами отсутствия сетевого нейтралитета, прозрачности и конфиденциальности.

При этом, нельзя не отметить, что наряду с полным пренебрежением принципов децентрализации в технологической части (в сетевой архитектуре), концепция Web 2.0 подняла на высокий уровень социальную активность, тем самым стимулировав формирование и развитие культуры социальной децентрализации. Пользователи Интернет получили возможность совместно участвовать и активно влиять на информационное наполнении Сети, формируя так называемую «редактируемую информационную паутину» и «коллективный разум».  Но, к сожалению, когда такие значимые социальные составляющие формируются на стороне каких-то централизованных (кем-то управляемых) сервисов, то это не более чем иллюзия свободы и децентрализации, которая в любой момент может стать идеальным инструментом для манипуляции сознанием масс, намного превосходящий по своим возможностям любые другие медиа, когда либо созданные за всю историю человечества.

!!! cite ""
    "Хотя лидеры отрасли часто стимулируют позитивные изменения, мы должны остерегаться концентрации власти, поскольку они могут сделать Сеть хрупкой"

    &mdash; [Sir Tim Berners-Lee][1]

И возможно, если бы трафик ограничивался только текстом и картинками, то вероятно рассуждения на тему «децентрализации» и «централизации» не имели бы сейчас существенного практического значения для большинства обычных пользователей Интернет, и вероятно полностью бы игнорировались бизнесом.  Производительность Сети с таким контентным наполнением, скорей всего, полностью удовлетворяла бы потребности большинства ее пользователей, а проблемы централизации находились бы исключительно в социальной плоскости и правовых аспектах о защите прав пользователей, достоверности и конфиденциальности информации, а также в вопросах о информационном контроле и монополизации Сети корпорациями. Но, появление в сети тяжелых форматов контента (видео, аудио, игр, и др), наряду с их популяризацией и значительным ростом эксплуатации Сети в режиме сквозной передачи данных при централизованном онлайн вещании (VoD и Live Stream), начали активно приводить к потери существенной части потенциала и производительности Сети, ведя к угрозе ее целостности. И последствия этого уже напрямую касаются всех, и поэтому заинтересованность в устранение этих проблем должна быть тоже у всех!


!!! cite ""
    "Мы рискуем потерять все, что получили от Интернета до сих пор, и все великие достижения, которые еще впереди. Будущее Интернета зависит от того, будут ли обычные люди брать на себя ответственность за этот необычный ресурс и бросать вызов тем, кто стремится манипулировать сетью против общественного блага"

    &mdash; [Sir Tim Berners-Lee][1]


## ВИДЕО МЕНЯЕТ ИНТЕРНЕТ!

Ненасытный аппетит к цифровому видео - в предпочтении к VoD (Видео по Запросу) и Live Stream (прямой эфир) - сильно изменили Интернет!

Несколько факторов, сыгравшие существенную роль в росте потребления видеотрафика:

- Появление популярного контента OTT от Hulu, Netflix, Amazon, YouTube, Twitch и других
- Повышенная популярность событий в прямом эфире, таких как спортивные мероприятия и концерты.
- Популяризация и активный рост размещения в Сети любительских роликов и web-трансляций.
- Постоянно растущие видеоролики с высоким разрешением - HD, 4K, 8K, 360, VR и другие.
- Значительный рост подключенных к Сети главных экранов домов (TV, STB) и мобильных устройств.
- Более высокая скоростью соединения с Интернетом
- Увеличение количества операторов, охватывающих доставку содержимого первого экрана через IP, а не традиционным QAM

**Сейчас!**

Уже практически никто не может себе представить Интернет без видео и видео без Интернета!

Объем видеотрафика в сети Интернет, суммарно по всем формам потребления (VoD, Live Stream, P2P, HTTP Sharing, IP-телевидение, и др.), составил около 90% от всего существующего трафика!

По оценки Nielsen, потребление видео составило свыше 5 часов в день на одного человека.

![alt_text]({{ assets_root }}/images/intro/image3.png "image_tooltip")


При этом, никогда раньше не было у зрителей так много вариантов подключения к потоковому контенту на их телевизоре. Является ли это совместимым мультимедийным устройством (например, Apple TV, Google Chromecast, Amazon Fire TV, Roku и т.п.), игровой приставкой или интеллектуальным телевидением. Появление таких устройств удовлетворяет желание потребителей получать доступ к контенту нажатием одном кнопки, при гораздо большем разнообразии выбора контента и с более широкими функциональными возможностями и удобствами (Пауза, TimeShift, Архивы TV-программ, SVoD, и мн. др)

Несколько миллиардов всевозможных устройств с поддержкой 4K уже продано и проникновение телевизоров 4K по миру достигло свыше 50%.

!!! info ""
    Руководители Facebook говорят о завершении письменного слова как части бизнес-плана, и что будущее социальной сети связано с видео и все более и более захватывающими видеоформатами, такими как VR 360. Как сказал Мендельсон: «Текст не исчезнет полностью», добавив «Вам придется писать для видео» и «Если бы я делал ставку, я бы сказал: видео, видео, видео».

    [https://qz.com/706461/facebook-is-predicting-the-end-of-the-written-word/](https://qz.com/706461/facebook-is-predicting-the-end-of-the-written-word/)

В итоге, Интернет просто обречен быть современной видеоплатформой для глобального вещания (от любительских видеороликов до профессионального контента)!

Но возникает вопрос: _Насколько Интернет, в существующем централизованном режиме эксплуатации может удовлетворить реальный спрос в видеосмотрении и текущие потребности ее пользователей?_

!!! info ""
    Мария Фаррелл, бывший старший исполнительный директор ICANN, организации, которая управляет системой доменных имен в Интернете, говоря о проблемах вызванных централизацией сети Интернет, указывает, что картина в значительной степени скрыта для обычного человека. Среднестатистический пользователь думает, все здорово, я могу смотреть футбол», - говорит она.  Но, тут возникает «Но…»?!

Централизованное интернет-вещание имеет невероятно ограниченные пропускные способности, несоизмеримые затраты и значительно уступает по уровню качества и стабильности прямых трансляций, относительно таких каналов дистрибуции как кабельные и спутниковые сети, что делает такое вещание практически не конкурентноспособным (малопригодным для дистрибуции дорогостоящих и востребованных прямых трансляций спортивных событий и другого линейного контента).

Таким образом, преодоление технологического разрыва между ограниченными возможностями инфраструктуры Интернета и требованиями к ее производительности, имеет решающее значение для дальнейшего роста и успеха Интернета и его жизнеспособности, как для обычных пользователей, так и для бизнеса.


## Вызов ВСЕМУ ИНТЕРНЕТУ!

### 1. Невероятный дефицит пропускной способности Сети Интернет

Международная пропускная способность, на 5 мая 2021 года, составляет 2000 Tbps, две трети из которой используются Google, Facebook, Amazon и Microsoft. (по данным [TeleGeography](https://blog.telegeography.com/2021-international-bandwidth-trends-demand-global-networks))

Пропускные возможности тройки крупнейших международных операторов CDN, которые открыто опубликовали данные: Cloudflare - 20 Tbps; Fastly Inc - 25 Tbps; Limelight Networks – 35 Tbps

<p style="text-align: right">
<em>Это очень и очень много! </em></p>


Однако, если бы кто-то захотел провести глобальную прямую трансляцию, в качестве от 720p до 2160p (4K), HDR, с частотой 60 кадров (см. [https://support.google.com/youtube/answer/1722171](https://support.google.com/youtube/answer/1722171)), используя существующие стандартные (Unicast) технологии, то получился бы казус!

Задействовав всю пропускную способность международного интернета и тройки крупнейших операторов CDN , только около 60 млн. зрителей (около 1,3% от всех пользователей Интернета) смогли бы одновременно посмотреть такую прямую трансляцию

Увы, но даже у интернета есть свои ограничения!

!!! info ""
    Возьмем, к примеру, церемонии открытия Олимпийских игр, которые по линейному (кабельному/спутниковому) ТВ смотрят свыше 3 млрд. человек!

    Для того, чтобы все эти зрители смогли в Интернете посмотреть открытие олимпиады в хорошем (указанном выше) качестве, при использовании стандартных технологий (unicast), пропускная способность и мощности сети Интернет (включая операторов CDN) должны быть в 50 раз больше существующей!

По оценке Akamai, в prime-time, количество зрителей желающих посмотреть спортивные интернет-трансляции, в прямом эфире, достигает 500 миллионов. «С 500 миллионами онлайн-зрителей нам нужно 1500Tbps (для обеспечения вещания трансляции с битрейтом 1,5 mbps, что в 10 раз меньше, чем необходимо для трансляции в HD1080p ). Сегодня мы делаем 32Tbps, поэтому вы можете увидеть огромный пробел, который мы должны преодолеть», - [говорит директор по выпуску продуктов и маркетингу Akamai, Ян Мэнфорд.](http://www.streamingmedia.com/Articles/ReadArticle.aspx?ArticleID=108594&PageNum=2)

В итоге, чтобы удовлетворить реальный спрос и потребности пользователей сети Интернет, исходя из расчета одновременного подключении к Сети 70% пользователей Сети, для просмотра онлайн-видео, в качестве [от 720p до 2160p (HDR, с частотой 60 кадров),](https://support.google.com/youtube/answer/1722171) при равнопропорциональном распределении смотрения по указанным форматам, необходимость в пропускной способности Интернет будет составлять свыше 100000 Tbps

Резюме:

**Дефицит пропускной способности Сети Интернет: - свыше 100000 Tbps**

Для удовлетворения текущих потребностей пользователей Интернет в онлайн-смотрении, необходимо увеличение существующих пропускных способностей Сети более чем в 50 раз!!!


### 2. Интернет не предоставляет никаких гарантий надежности и производительности при ее эксплуатации в режиме централизованной доставки данных

Разработанная как открытая, равноправная и децентрализованная платформа, сеть Интернет не предоставляет никаких гарантий надежности и производительности при ее эксплуатации в режиме централизованной доставки данных. Напротив, широкополосные интернет-коммуникации при сквозной передаче данных подвержены ряду узких мест, которые отрицательно влияют на производительность, включая задержку, потерю пакетов, сбои в сети, неэффективные для таких целей протоколы и межсетевые проблемы.

Теперь, более детально:

!!! info ""
    Хотя Интернет определяется как единое целое, сейчас он фактически состоит из тысяч различных сетей, каждая из которых обеспечивает доступ к небольшому проценту конечных пользователей. Даже в самой большой сети покрытие составляет не более 5% доступного трафика Интернет (см. рис).

    ![alt_text]({{ assets_root }}/images/intro/image4.png "image_tooltip")

Фактически, для достижения 90% охвата требуется задействовать более 650 сетей. Это означает, что контент, размещенный на центральном сервере, должен перемещаться по нескольким сетям, чтобы его в итоге получили конечные пользователи.

На практике, межсетевой обмен данными не является эффективным и надежным, так как на него могут оказывать неблагоприятное воздействие ряд факторов. Наиболее значимыми являются:

**Перегрузки на точках пиринга (обмена трафиком)** (Peering point congestion).

Производительность на точках пиринга, где происходит обмен сетевым трафиком, обычно сильно отстает от спроса, что во многом обусловлено экономической структурой Интернета. Деньги аккумулируются на «первой миле» (то есть, на веб-сайте) и на «последней миле» (то есть, у конечных пользователей), стимулируя тем самым инвестиции в инфраструктуру первой и последней мили. При этом, существует мало экономических стимулов для инвестиций в «среднюю милю, так как это большие расходы, с нулевым уровнем дохода, где сети еще и вынуждены сотрудничать с конкурирующими организациями. Таким образом, эти точки обмена становятся узкими местами, которые вызывают потерю пакетов и увеличивают латентность.

**Неэффективные протоколы маршрутизации**.  (Inefficient routing protocols)

Протокол маршрутизации BGP (Border Gateway Protocol ). Несмотря на то, что он отлично справился с масштабированием Интернета, BGP имеет ряд ограничений. Он не был разработан для обеспечения эффективной производительности. Например, несколько маршрутов между местоположениями в Азии фактически направляются через точки пиринга в США, что значительно увеличивает задержку. Кроме того, когда маршруты перестают работать или ухудшаются возможности подключения, BGP может медленно формировать новые маршруты.  BGP основывает свои вычисления маршрутизации на основе информации, предоставленной соседями в киберпространстве, которые, в свою очередь, собирают информацию от своих соседей в киберпространстве и т. д., ничего не зная о топологиях, задержках или перегрузке основных сетей в режиме реального времени. Это хорошо работает, пока информация, содержащаяся в сообщениях BGP, называемых “advertisements”, является точной. Любая ложная информация может распространяться почти мгновенно через Интернет, потому что нет возможности проверить честность или даже личность тех, кто отправляет такие сообщения. Обычные человеческие ошибки или преднамеренные  действия злоумышленников; неправильно сконфигурированные или захваченные маршруты могут быстро распространяться по всему Интернету, вызывая разворот маршрута, раздутые пути и даже широкомасштабные отключения связи.

Примеры захвата: В 2008 году правительство Пакистана решило заблокировать на территории страны YouTube из-за видео с пророком Мухаммедом. Ошибочные команды в BGP привели к тому, что трафик большинства пользователей YouTube перенаправлялся в Пакистан, из-за чего сайт не работал около двух часов. В 2010 году из-за некорректной команды, отправленной китайским телекоммуникационным оператором China Telecom, трафик военных ведомств США около 18 минут проходил через Китай — и ничто не мешало его перехватывать. Наконец, в 2014 году злоумышленник перенаправил в Канаду трафик 19 провайдеров, для того чтобы украсть биткоины. И это только несколько из случаев, но чаше всего на сбои в работе BGP просто не обращают внимания.

При этом роль BGP нельзя недооценивать, из-за того, что он не вписывается в картину централизованного интернета. Благодаря этому протоколу Интернет остаётся децентрализованной глобальной сетью и продолжает расти!

**Ненадежные сети**.

В Интернете все время происходят сбои в работе, вызванные самыми разными причинами: отключением кабелей, неправильно сконфигурированными маршрутизаторами, атаками DDoS, перебоями в подаче электроэнергии, даже землетрясениями и другими стихийными бедствиями. В то время как сбои различаются по масштабу, крупномасштабные события не являются редкостью. В некоторых случаях на устранение неполадок может уходить несколько дней.

**Неэффективные протоколы связи**.

Протокол TCP, несмотря на то, что он был разработан для обеспечения надежности и предотвращения перегрузок, он несет значительные накладные расходы и может иметь субоптимальную производительность для ссылок с высокой задержкой или потерей пакетов, которые являются общими для широкополосного Интернета. Проблема с перегрузками в средней миле усугубляет проблему, поскольку потеря пакетов вызывает повторную передачу TCP, что еще более замедляет обмен данными.

TCP является серьезным узким местом для видео и других больших файлов. Поскольку для каждого отправленного пакета данных требуется подтверждение, пропускная способность (при использовании стандартного TCP) зависит как от латентности сети, так и от RTT (round-trip time). Таким образом, расстояние между сервером и конечным пользователем может стать основным узким местом в скорости загрузки и в качестве просмотра видео. Таблица, приведенная ниже, иллюстрирует резкие результаты этого эффекта. Например, онлайн потоки в HD невозможны, если сервер не находится поблизости.


<table>
  <tr>
   <td>Distance
<p>
(Server to User)
   </td>
   <td>Network RTT
   </td>
   <td>Typical Packet Loss
   </td>
   <td>Throughput
   </td>
   <td>4GB DVD Download Time
   </td>
  </tr>
  <tr>
   <td>Local:
<p>
&lt;100 mi.
   </td>
   <td>1.6 ms
   </td>
   <td>0.6%
   </td>
   <td>44 Mbps (high quality HDTV)
   </td>
   <td>12 min.
   </td>
  </tr>
  <tr>
   <td>Regional:
<p>
500–1,000 mi.
   </td>
   <td>16 ms
   </td>
   <td>0.7%
   </td>
   <td>4 Mbps (basic HDTV)
   </td>
   <td>2.2 hrs.
   </td>
  </tr>
  <tr>
   <td>Cross-continent:
<p>
~3,000 mi.
   </td>
   <td>48 ms
   </td>
   <td>1.0%
   </td>
   <td>1 Mbps (SD TV)
   </td>
   <td>8.2 hrs.
   </td>
  </tr>
  <tr>
   <td>Multi-continent:
<p>
~6,000 mi.
   </td>
   <td>96 ms
   </td>
   <td>1.4%
   </td>
   <td>0.4 Mbps
<p>
(poor)
   </td>
   <td>20 hrs
   </td>
  </tr>
</table>


**Экономическая масштабируемость.**

Экономическое масштабирование - означает наличие достаточного количества ресурсов для удовлетворения мгновенного спроса, будь то во время запланированных событий или неожиданных периодов пикового трафика. Масштабирование централизованной инфраструктуры является дорогостоящим и трудоемким процессом, и трудно заранее прогнозировать потребности в мощности. Отсутствие необходимых ресурсов у вещателей приводит к разочарованию потребителей и потенциальной потери бизнеса, а чрезмерное превышение означает трату денег на неиспользуемую инфраструктуру.

При этом, нужно хорошо понимать, что масштабируемость означает не только обеспечение достаточной производительности и пропускной способности исходящих серверов, но и достаточную пропускную способность по всей сети, ко всем конечным пользователям/потребителям.  Это очень и очень серьезная проблема!  Устранение этой проблемы сейчас является самой насущной задачей, решением которой активно занимаются все компании, желающие использовать Интернет в качестве профессиональной системы дистрибуции контента


### 3. Большие расходы на осуществление онлайн-трансляций

Для относительно нормальной работы сервисов OTT и качественных трансляций (VoD и Live Stream) необходимо создание собственной инфраструктуры CDN или использование сторонней.

!!! info ""
    В архитектуре CDN берется оригинальный медиа-контент и копируется на сотни или тысячи своих серверов, которые устанавливаются по всему миру. Поэтому, когда, скажем, вы входите в систему из Амстердама, вместо того, чтобы подключаться к основному серверу вещателя находящемуся в Соединенных Штатах, он будет загружать такую ​​же копию с сервера Амстердама, или который ближе всего к Амстердаму. Это не дает нагрузку на международные и межрегиональные сети, и за счет сокращения расстояния между сервером и клиентом,  делает связь более стабильной и значительно ускоряет передачу данных (увеличивает скорость загрузки конечным пользователям).

    Именно CDN являются причиной того, что трансляции сервисов с огромным количеством пользователей, таких как YouTube , Twich, Facebook и др., могут сейчас смотреть пользователи в разных странах мира, в хорошем качестве и с минимальными задержками.

Создание собственной высококачественной инфраструктуры CDN, соответствующей по уровню и пропускной способности, к примеру Akamai, обойдется в несколько млрд.$ +  миллионные ежемесячные расходы на содержание, а для ее разворачивания понадобиться не мало времени, и все это для того что бы иметь возможность осуществить одну глобальную интернет-трансляцию в HD-качестве (10 mbps) на 10 млн. одновременных зрителей, находящихся в разных точкам мира (не определенной географии).  При этом, будет еще и насущно стоять вопрос экономической масштабируемости (планирование мощности, гибкость, качество), проще говоря: нужно будет решить задачу, чтобы чрезмерное предоставление ресурсов не приводило к лишним расходам, а недостаточное не приводило к низкому качеству и перебоям в обслуживании, и проблема заключается в том что сложно спрогнозировать, будет ли трафик по определенным событиям увеличиваться в 2 или 10 раз выше обычных объемов. В итоге, такое решение (создание собственной CDN) актуально только для крупнейших международных операторов ОТТ (к примеру, таких как YouTube, Netflix  и т.п.), и абсолютно не подходит для международных сервисов не имеющих такую большую аудиторию и также не подходит организаторам вещания каких-то нечастых масштабных событий, к примеру таких как популярные спортивные события, концерты, природные или техногенные катастрофы или важные местные или мировые события, и т.д. и т.п.

!!! info ""
    Один интересный факт: Netflix насчитывающий около 100 млн. пользователей из 190 стран изначально создал собственную инфраструктуру, из огромного парка серверов. В то время как инженеры Netflix написали сотни программ и развернули их на своих серверах, обеспечивая работу более 700 микросервисов, управляющих каждой из многих частей того, что составляет инфраструктуру Netflix, и в итоге оказалось, что всего этого недостаточно, чтобы получить необходимую производительность.  И для увеличения производительности, они приняли решение подключить инфраструктуру Amazon Web Services (AWS), а в последующем, помимо AWS, для увеличения производительности также были еще подключены сети еще нескольких коммерческих операторов CDN, таких гигантов как Akamai, Level 3 и Limelight Networks. Но и этого оказалось недостаточно, но об этом позже (о проблеме «последней мили»)

При задействовании сторонних операторов CDN, таких как Akamai и т.п, необходимо ориентироваться на их свободные ресурсы (с необходимостью их предварительного резервирования для трансляции каких-то масштабных событий) и стоимость их услуг. К примеру: при средней цене услуги CDN – 0,05$ за 1 Гб, стоимость 1 часа трансляции, для 1 млн. одновременных зрителей, в HD (с битрейтом 10 mbps), будет обходится вещателю в размере – 225 000$.  При этом, проблема ограниченных пропускных возможностей существующих стандартных операторов CDN никуда не девается.



### 4. Проблема «последней мили», без решения которой невозможно обеспечить высококачественные уровень трансляции

«Последняя миля» — часть сети, которая физически достигает устройства конечного пользователя (канал, соединяющий конечное (клиентское) устройство с узлом ISP)

В архитектуре, где доставка контента осуществляется исключительно через централизованную CDN и нет своего физического присутствия на «последней миле» (нет прямого доступом к сети ISP, к которой подключен Зритель), нет возможности обеспечения гарантированной и стабильной скорости передачи данных, соответствующей уровню сетевого подключения клиента  к своему Интернет-провайдеру (ISP). К примеру, наличие порта 100 Мбит/с не означает данную скорость на всех участках сети, так как свободная пропускная способность магистрального канала в момент передачи данных может быть всего 10 Мбит/с. Поэтому, чтобы избавиться от претензий со стороны клиентов, вещателю приходится оправдываться перед своими клиентами/зрителями, заявляя, что у них было бы все хорошо с воспроизведением контента,  если бы их Интернет-провайдер был напрямую подключен к сети CDN, обслуживающей сервис/вещателя.

!!! info ""
    В этом случае, к примеру, чтобы избавится от претензий со стороны клиентов имеющих 4К телевизоры и подключения к сети Интернет  на скорости 100 мбит/с, которые не могут понять, почему им приходится смотреть видео в низком качестве, с битрейтом на уровне 2 мбит/с, вместо заявленного 4К), Netflix поддерживает свой индекс скорости Интернет-провайдеров, создав для этого специальную страничку на своем ресурсе ([ispspeedindex.netflix.com](https://ispspeedindex.netflix.com/)), и постоянной периодичностью еще и публикует индексы скоростей в своем  блоге, благодаря чему потребители могут видеть, какие скорости для потоков Netflix  предоставляют их провайдеры.

Существует множество причин, по которым сервера CDN размещают в больших централизованных региональных центрах обработки данных, а не в очень небольших точках присутствия (points of presence, PoP). Большие библиотеки контента, используемые для доставки VoD, вполне нормально для этого подходят. Тем не менее, для обеспечения высокого качества и стабильности линейного контента (прямого вещание) или популярных выпусков VoD, необходимо доставлять контент из ресурсов CDN, расположенных в небольших PoP, и находясь как можно глубже в сети, «ближе к зрителю». Как правило, операторы должны идентифицировать поставщиков, которые могут помочь им обеспечить устойчивое высокое качество, а также повысить гранулярность и гибкость.

!!! info ""
    Джо МакНаме, исполнительный директор европейской правозащитной группы EDRi, отметил, что даже если владельцы видео-сервисов строят свою собственную параллельную инфраструктуру (CDN), то их усилия интеграции по вертикали не смогут распространяться на контроль «последней мили». А если нет контроля «последней мили», то нет гарантии стабильности потоков для конечных пользователей Интернет (Зрителей).

Однако по затратам и с точки зрения программного обеспечения и оперативной сложности может возникнуть сложность развертывания и управления множеством широко распространенных серверов. Задача заключаться в том, что вещателям и их  CDN необходимо найти способ размещения своих выделенных или виртуальных серверов «ближе к зрителю», а этой точкой уже является инфраструктура ISP (Интернет-провайдер, услугами которой пользуется Зритель), и это уже означает что нужно индивидуально договариваться об условиях с каждым ISP и нести дополнительные расходы.

!!! info ""
    К примеру, для решения этой задачи, Netflix реализовал проект под название Open Connect, суть которого заключается в создании еще одного поверхностного слоя CDN, с прямым проникновением в инфраструктуру Интернет-провайдеров (ISP).  [https://openconnect.netflix.com](https://openconnect.netflix.com)

Также, очень важно понимать, что при наличии у вещателя большой аудитории зрителей и трафика который будет составлять значительный % от общего объема всего трафика сети какого-либо ISP, то существует вероятность риска урезания такого трафика Интернет-провайдером, что существенно скажется на качестве трансляций, а в некоторых случаях может даже произойти полная блокировка доступа к такому сервису вещателя, и ISP затребует от вещателя какую-то компенсацию, что в итоге также приведет к дополнительным расходам, даже если они изначально не планировались.

Возможно, для тех кто думает что Интернет-провайдерам не должно быть  до этого никакого дела, и их вообще не должно интересовать что и в каком объеме передают его абоненты,  такой аргумент покажется незначительным.  Но, в реалиях, это является очень весомым аргументом, так как по факту абонентам не предоставляются гарантированные выделенные интернет-каналы, и именно благодаря умелому перераспределению трафика, Интернет-провайдеры делают свой бизнес, а абоненты получают недорогие тарифные планы.

!!! info ""
    Показательным примером реальности и актуальности обозначенной проблемы может послужить конфликт, возникший между Netflix и ISP Verizon. Суть конфликта заключалось в том, что Verizon  заблокировал своим клиентам услуги Netflix, заявил, что Netflix злоупотреблял своим положением в качестве супертяжелого отправителя интернет-трафика (составлявшего 30% пикового трафика на то время), и потребовал у Netflix  компенсацию за предоставления такой полосы.

По факту, как мы видим, чтобы вещателю достичь хорошего уровня производительности в централизованной архитектуре, нужно практически построить свою частную, отдельную сеть Интернет. И это есть одним из возможных вариантов решения. Но кто такое решение может себе позволить и к чему это приведет?!

В случае, если же услуги ISP начнут массово предоставлять непосредственно компании владеющими всевозможными собственными сервисами VoD, Live Stream (к примеру, такие как Google, Facebook, Amazon, Apple и т.п.), а именно к этому они и стремятся (видя в этом один из вариантов оптимального решения проблемы «последней мили» и возможности полного  контроля серфинга пользователей в Сети), то тогда еще и возникнет конфликт бизнес-интересов. Пояснять, какому трафику будет отдаваться предпочтение в такой ситуации, вероятно будет лишним. В этом случае можно будет забыть о каком-либо сетевом нейтралитете, и соответственно единственным кто сможет обеспечить высокие гарантии качества и стабильности трансляции, это будет тот, кому принадлежит ISP!

Хотите ли Вы чтобы вам когда-то пришлось делать выбор между Netflix или YouTube или Apple TV и  др., из понимания того, что делая выбор в пользу какого-то ISP, вы сможете нормально пользоваться только одним из этих сервисов?!  Нужен ли Вам интернет от Google, чтобы не иметь проблем с работой его сервисов или интернет от Facebook, чтобы нормально пользовать его сервисы, и нужен ли Вам вообще такой Интернет, где нужно делать такой выбор?!

!!! cite "Джо МакНаме"
    Что произойдет, когда у вас есть огромная инфраструктура CDN и доступ к последней миле? Вы попадаете в другой мир

Эта ситуации демонстрирует насколько серьезными и большими могут быть проблемы у вещателей, не имущих доступ к «последней мили», и насколько мрачной может быть ситуация когда у кого-то есть монопольная власть над «последней милей».


### 5. Низкий уровень стабильности потока

Размещение серверов в непосредственной близости от конечных пользователей значительно увеличивают стабильность и исходящую пропускную способность всей системы, но это полностью не решает задачу по достижению уровня качества и стабильности, не уступающего таким каналам дистрибуции, как спутниковое и кабельное телевидение.

Проблема заключается в том, что при использовании централизованной технологии (Unicast) Зритель получает поток только от одного единственного сервера, что в совокупности с ранее обозначенными проблемами и ограничениями, очень существенно может влиять на стабильность трансляции, в случае каких-либо сбоев на стороне сервера или на маршруте.

Стандартные Unicast-технологии не позволяют реализовывать сложные сценарии обработки данных и перераспределения видеопотоков так, чтобы в случае проблем с каким-либо из узлов или маршрутом, это не вызывало у Зрителя задержку прямой трансляции или не ухудшило ее качество, а соответственно при использовании такой технологии не может быть никаких гарантий стабильности потока.

Проше говоря, чтобы конкурировать с такими каналами дистрибуции, как спутниковое и кабельное ТВ, то единственной причиной остановки трансляции должно быть только отсутствие/отключение Интернета!

Добиться соответствующих гарантий стабильности потока, можно только посредством технологии позволяющей осуществлять загрузку данных в несколько потоков, одновременно от нескольких серверов и по разным маршрутам, что будет полностью нивелировать возможные проблемы на каком-то из серверов или маршрутов. Но такая реализация - это уже будет не Unicast, а P2P, и это уже совсем другая история.


### 6. Огромный экологический урон от централизованной стриминговой индустрии

Около 90% ресурсов Интернета и дата-центров работают на стриминговую индустрию, чтобы обеспечить около 1% от реальных потребностей пользователей в онлайн-смотрении. Соответственно, чтобы покрыть дефицит пропускной способности стандартными методами, нужно в сотни раз увеличить мощности централизованной инфраструктуры. При этом дата-центры уже сейчас используют более 2% мировой электроэнергии и производят такое же количество углеродных выбросов **(свыше 650 млн т/год. CO<sub>2</sub>), **что уже в пять раз превосходит уровень объемов выбросов CO<sub>2</sub>  при майнинге Bitcoins**! **

!!! info ""
    При самых энергоэффективных технологиях, согласно оценкам экспертов, в течение десятилетия на дата-центры и цифровую инфраструктуру будет приходиться уже до 20% мирового потребления электроэнергии и 5,5% выбросов CO<sub>2</sub>


<p style="text-align: right">
<a href="https://thenextweb.com/news/data-centers-generate-the-same-amount-of-carbon-emissions-as-global-airlines">Ссылка на TNW</a></p>

## Как обстоят ДЕЛА НА ПРАКТИКЕ?

На практике, как обычно, - все намного сложнее и менее предсказуемо, чем в теории!

Чтобы лучше понять, насколько обозначенные сетевые проблемы реально влияют на трансляции и насколько эффективно или неэффективно работают существующие стандартные решения (unicast), ну а также насколько это может быть значимо, особенно для прямых трансляций спортивных событий, приведем несколько исторических примеров:

Очень значимое для компании Amazon событие, такое как первая прямая трансляция спортивного матча прошла с серьезными проблемами (матч [Chicago Bears-Green Bay Packers](http://www.multichannel.com/news/content/amazon-s-nfl-stream-solid-not-perfect/415594) )

После Amazon, трансляцию следующего матча  провела Yahoo , и тоже возникли проблемы ([матч NFL, между Baltimore Ravens и Jacksonville Jaguars](http://www.multichannel.com/news/sports/complaints-about-yahoo-s-nfl-live-coverage-stream/415461))

И с проблемами проведения масштабных прямых трансляций сталкиваются практически все компании, включая Apple (многие до сих пор помнят провал трансляции презентации нового IPhone 6 Plus, который практически открывал новую эру смартфонов).

Ну и наверно весомым и неоспоримым показателем существующих реалий послужит результат очень значимого в мире спортивного события, с огромным количеством желающих его посмотреть в прямом эфире, и за просмотр которого его эксклюзивный вещатель в США запросил 100$. Возможно, мало у кого будет укладываться в голове, что могут возникнуть какие-то проблемы с трансляцией, за просмотр которой платят по 100$?!

27 августа 2017 г., в Лас-Вегасе прошел поединок, названный самый ожидаемым поединком столетия и самым знаковым событием современного бокса — 12-кратный чемпион мира по боксу Флойд Мейвезер-младший сошелся на ринге с бойцом смешанных единоборств Конором МакГрегором.  Прямая трансляция велась по системе pay-per-view (плати за просмотр), с ценою за просмотр в обычном качестве -  $89.95, в HD– $99.95.

Результат : [Разочарование огромного количества людей!](http://ftw.usatoday.com/2017/08/mayweather-vs-mcgregor-streaming-ufc-tv-ppv-pay-per-view-time-online-watch)

Все-таки, прямые трансляции – это вам не статичные видеофайлы раздавать (не кино крутить)!

И если такие проблемы с прямыми трансляциями наблюдаются у самых крупнейших интернет-гигантов, обладающими многомиллиардными инфраструктурами CDN, да еще и с четко прогнозируемым количеством одновременных зрителей, при невероятно высокой стоимости доступа к просмотру трансляции, то возникает вопрос: - можно ли вообще посредством Интернет обеспечить высококачественную стабильную трансляцию, да еще и на неограниченное (не прогнозируемое) количество зрителей?

Прежде чем получить ответ на поставленный вопрос, важно отметить, что по факту «проблемы» не в самой сети Интернет, а в том как ее эксплуатируют. Все обозначенные выше проблемы не имеют никакого значения для одноранговых (peer-to-peer, P2P) сетей с децентрализованной системой хранения и доставки данных, что явно указывает, насколько противоестественной для сети Интернет является централизация, и насколько трудно достичь приемлемых уровней производительности, надежности и экономической масштабируемости, при таком режиме ее эксплуатации.

**Вот такая она реальность!**

Теперь вы уже точно знаете, где находятся ограничения и насколько могут существующие пропускные способности сети Интернет удовлетворить реальные потребности в Интернет-смотрении, при использовании стандартных (Unicast) технологий онлайн-вещания!

## Однако решение есть!

Все что нужно, так это наконец-то начать использоваться Сеть именно так, как ее и предполагалось использовать изначального (когда она создавалась), с учетом ее архитектуры, акцентированной на одноранговость (P2P, Peer to Peer) и максимальную децентрализацию

### Вкратце, о технологии P2P

Peer-to-peer, или сокращенно P2P сеть, - это вид компьютерных сетей, использующих распределенную архитектуру. Это означает, что все компьютеры или устройства, входящие в нее, предоставляют и используют ресурсы совместно. Компьютеры или устройства, которые являются частью пиринговой сети, называются пирами. Каждый узел одноранговой сети, или пир, равен другим пирам. Привилегированных участников нет, как и нет центрального административного устройства. Таким образом, и только при таких условиях, сеть считается децентрализованной.

В некотором роде одноранговые сети - это социалистические сети в цифровом мире. Каждый участник Сети равен другим, и каждый имеет те же права и обязанности, что и другие. Каждое устройство (узел/peer) подключенные к одноранговой Сети является как клиентом, так и выполняет функции сервера, тем самым не только потребляя ресурсы Сети (других участников), а еще и предоставляя Сети (другим участникам) свои собственные ресурсы (аппаратные и сетевые). Каждый ресурс, доступный в пиринговой сети, является общим для всех узлов без участия центрального сервера. Общими ресурсами в сети P2P могут быть:

- Процессорные мощности
- Дисковое пространство
- Пропускная способность сети

### Принцип работы P2P-сети

Основная цель одноранговых сетей заключается в совместном использовании ресурсов и совместной работе компьютеров и устройств, предоставлении конкретной услуги или выполнении конкретной задачи. Как упоминалось ранее, децентрализованная сеть используется для совместного использования всех видов вычислительных ресурсов, таких как вычислительная мощность, пропускная способность сети или дисковое пространство.

Наиболее распространенным вариантом использования пиринговых сетей является обмен файлами в Интернете. Одноранговые сети идеально подходят для обмен файлами в Интернете (распределенного хранения и передачи файлов), поскольку они позволяют подключенным к ним компьютерам получать и отправлять файлы одновременно, используя многочисленное количество источников.

Рассмотрим ситуацию: посещая видеохостинг (к примеру, как YouTube), вы загружаете видеофайл (да, если вы не знали, то просматривая любое онлайн-видео в Интернете вы всегда загружаете видеофайл на свое устройство). В этом случае видеохостинг работает как сервер, а ваше устройство выступает в роли клиенте, получающего видеофайл с сервера видеохостинга. Вы можете это сравнить с односторонним движением по однополосной дороге: загружаемый файл - это предназначенный для вас груз, который везет вам один единственный автомобиль, который идет от точки A (видеохостинг) до точки B (ваше устройство), причем по однополосной дороге, где самая незначительная авария или пробка на дороге приводит к значительной задержке доставляемого вам груза, а в случае серьезной аварии или пробки на дороге, груз повторно отправляется по новой дороге, но уже по другому маршруту, где могут возникнуть те же самые проблемы, что в итоге может привести к значительным ухудшениям качества такого сервиса и ущербу доставляемого груза (остановкам на буферизацию, снижению визуального качества онлайн-видео или невозможности его воспроизведения вообще).

Если вы загружаете один и тот же файл через одноранговую сеть, используя P2P-протокол для передачи данных, то загрузка файла выполняется по-другому. Файл загружается на ваше устройство по частям, одновременно от многих других устройств, у которых уже есть необходимые вам части файла, а скаченные вами куски файла (обычно размером в 16 кб, к примеру, как для протокола BitTorrent) объединяются в нужный вам исходный файл уже непосредственно на вашем устройстве. В то же время части этого файла также отправляются (загружаются) с вашего устройства другим участникам Сети, которые его запрашивают. Эта ситуация похожа на двухстороннюю дорогу, с не ограненным количеством полос, да еще и с одновременной доставкой данных (частей файла) по разным маршрутам:  нужный вам файл (груз) многократно дублируется, разделяется на части, грузиться на многочисленные авто и отправляется вам одновременно по нескольким маршрутам, где дороги имеют неограниченного количества полос. В свою очередь, вы также становитесь активным участником такого логистического движения, и уже по встречной полосе отправляете имеющиеся у вас в наличии груз (копии полученных вами частей файла) по запросу других участников (пользователей) Сети.

**Эталонная одноранговая (P2P) Сеть является неограниченной в масштабировании, без дефицита ресурсов и пропускных способностей, и не причиняет вред экологии!**

Наличие эффективного P2P-протокола и надежного комплексного решения, с возможностью его беспрепятственного внедрения и использования в системе доставки данных, поверх существующей инфраструктуры Интернет, — это и есть то единственное решение всех обозначенных проблем, относящихся к стандартным технологиям Unicast!

[1]: http://www.wired.co.uk/article/tim-berners-lee
[2]: http://info.cern.ch/
